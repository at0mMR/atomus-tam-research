{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atomus TAM Research - Debugging Tools\n",
    "\n",
    "This notebook provides interactive tools for testing and debugging the Atomus TAM Research system.\n",
    "\n",
    "## Features:\n",
    "- Individual API testing\n",
    "- Error diagnosis\n",
    "- Data exploration\n",
    "- Configuration validation\n",
    "- Performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our modules\n",
    "from api_integrations import (\n",
    "    create_hubspot_client,\n",
    "    create_openai_client, \n",
    "    create_highergov_client\n",
    ")\n",
    "from utils import (\n",
    "    get_logger,\n",
    "    get_performance_tracker,\n",
    "    log_system_info\n",
    ")\n",
    "from scoring_engine import AtomScoringEngine\n",
    "from data_processing import AtomDataProcessor\n",
    "\n",
    "print(\"âœ… All imports successful\")\n",
    "print(f\"ğŸ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger and get system info\n",
    "logger = get_logger()\n",
    "tracker = get_performance_tracker()\n",
    "\n",
    "print(\"ğŸ–¥ï¸ SYSTEM INFORMATION:\")\n",
    "log_system_info()\n",
    "\n",
    "# Check environment variables\n",
    "print(\"\\nğŸ”‘ ENVIRONMENT VARIABLES:\")\n",
    "env_vars = {\n",
    "    'HUBSPOT_API_KEY': os.getenv('HUBSPOT_API_KEY', 'Not set'),\n",
    "    'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY', 'Not set'),\n",
    "    'HIGHERGOV_API_KEY': os.getenv('HIGHERGOV_API_KEY', 'Not set')\n",
    "}\n",
    "\n",
    "for key, value in env_vars.items():\n",
    "    if value == 'Not set':\n",
    "        print(f\"âŒ {key}: {value}\")\n",
    "    else:\n",
    "        # Show only first 10 and last 4 characters for security\n",
    "        masked = f\"{value[:10]}...{value[-4:]}\" if len(value) > 14 else \"[MASKED]\"\n",
    "        print(f\"âœ… {key}: {masked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Individual API Testing\n",
    "\n",
    "Test each API separately to identify any connection issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HubSpot API\n",
    "print(\"ğŸ”µ TESTING HUBSPOT API...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    hubspot_client = create_hubspot_client()\n",
    "    print(\"âœ… HubSpot client created\")\n",
    "    \n",
    "    # Test connection\n",
    "    status = hubspot_client.test_connection()\n",
    "    print(f\"ğŸ“Š Connection Status: {status}\")\n",
    "    \n",
    "    # Test searching for companies\n",
    "    print(\"\\nğŸ” Testing company search...\")\n",
    "    search_results = hubspot_client.search_companies({'name': 'Test'})\n",
    "    print(f\"ğŸ“‹ Found {len(search_results)} companies matching 'Test'\")\n",
    "    \n",
    "    # Show current stats\n",
    "    print(\"\\nğŸ“ˆ HubSpot Stats:\")\n",
    "    hubspot_client.log_stats_summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ HubSpot API Error: {str(e)}\")\n",
    "    print(f\"ğŸ” Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI API\n",
    "print(\"ğŸ¤– TESTING OPENAI API...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    openai_client = create_openai_client()\n",
    "    print(\"âœ… OpenAI client created\")\n",
    "    \n",
    "    # Test connection\n",
    "    status = openai_client.test_connection()\n",
    "    print(f\"ğŸ“Š Connection Status: {status}\")\n",
    "    \n",
    "    # Test simple research\n",
    "    print(\"\\nğŸ” Testing basic research...\")\n",
    "    test_research = openai_client.conduct_research(\n",
    "        company_name=\"Firestorm\",\n",
    "        research_type=\"basic_research\",\n",
    "        research_category=\"company_overview\"\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“ Research completed: {len(test_research['content'])} characters\")\n",
    "    print(f\"ğŸ¯ Tokens used: {test_research['metadata']['tokens_used']}\")\n",
    "    print(f\"ğŸ“„ Preview: {test_research['content'][:200]}...\")\n",
    "    \n",
    "    # Show current stats\n",
    "    print(\"\\nğŸ“ˆ OpenAI Stats:\")\n",
    "    openai_client.log_stats_summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ OpenAI API Error: {str(e)}\")\n",
    "    print(f\"ğŸ” Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HigherGov API\n",
    "print(\"ğŸ›¡ï¸ TESTING HIGHERGOV API...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    highergov_client = create_highergov_client()\n",
    "    print(\"âœ… HigherGov client created\")\n",
    "    \n",
    "    # Test connection\n",
    "    status = highergov_client.test_connection()\n",
    "    print(f\"ğŸ“Š Connection Status: {status}\")\n",
    "    \n",
    "    # Test defense contractor analysis\n",
    "    print(\"\\nğŸ” Testing defense contractor analysis...\")\n",
    "    defense_analysis = highergov_client.analyze_defense_contractor_status(\"Firestorm\")\n",
    "    \n",
    "    print(f\"ğŸ“Š Defense Score: {defense_analysis['defense_contractor_score']}\")\n",
    "    print(f\"ğŸ“‹ Contract Count: {defense_analysis['contract_analysis']['defense_contracts']}\")\n",
    "    print(f\"ğŸ¢ Identifiers Found: {len(defense_analysis['identifiers'])}\")\n",
    "    \n",
    "    # Show current stats\n",
    "    print(\"\\nğŸ“ˆ HigherGov Stats:\")\n",
    "    highergov_client.log_stats_summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ HigherGov API Error: {str(e)}\")\n",
    "    print(f\"ğŸ” Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Configuration Testing\n",
    "\n",
    "Validate that all configuration files are properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Scoring Engine Configuration\n",
    "print(\"âš™ï¸ TESTING SCORING ENGINE CONFIGURATION...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    scoring_engine = AtomScoringEngine()\n",
    "    print(\"âœ… Scoring engine initialized\")\n",
    "    \n",
    "    # Show configuration summary\n",
    "    config = scoring_engine.get_config_summary()\n",
    "    print(f\"\\nğŸ“‹ Configuration Summary:\")\n",
    "    print(f\"   ğŸ“ Scoring weights loaded: {len(config['weights'])} categories\")\n",
    "    print(f\"   ğŸ”¤ Keyword categories: {len(config['keywords'])}\")\n",
    "    print(f\"   ğŸ¯ Tier thresholds: {config['tier_thresholds']}\")\n",
    "    \n",
    "    # Show keyword counts\n",
    "    print(f\"\\nğŸ”¤ Keyword Categories:\")\n",
    "    for category, keywords in config['keywords'].items():\n",
    "        print(f\"   {category}: {len(keywords)} keywords\")\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ Scoring Weights:\")\n",
    "    for category, weight in config['weights'].items():\n",
    "        print(f\"   {category}: {weight}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Scoring Engine Error: {str(e)}\")\n",
    "    print(f\"ğŸ” Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Processor\n",
    "print(\"ğŸ“‹ TESTING DATA PROCESSOR...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    data_processor = AtomDataProcessor()\n",
    "    print(\"âœ… Data processor initialized\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = data_processor.load_prospect_database()\n",
    "    print(f\"ğŸ“Š Loaded {len(test_data)} test companies\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if test_data:\n",
    "        print(f\"\\nğŸ“„ Sample company data:\")\n",
    "        sample = test_data[0]\n",
    "        for key, value in sample.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "            \n",
    "    # Test validation\n",
    "    validation_result = data_processor.validate_company_data(sample)\n",
    "    print(f\"\\nâœ… Validation result: {validation_result['is_valid']}\")\n",
    "    if not validation_result['is_valid']:\n",
    "        print(f\"âŒ Validation errors: {validation_result['errors']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Data Processor Error: {str(e)}\")\n",
    "    print(f\"ğŸ” Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§® Interactive Scoring Testing\n",
    "\n",
    "Test the scoring engine with individual companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Company Scoring\n",
    "print(\"ğŸ§® INTERACTIVE SCORING TESTING...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Define test company for scoring\n",
    "test_company = {\n",
    "    'name': 'Firestorm',\n",
    "    'description': 'Firestorm is a defense technology company specializing in hypersonic propulsion systems and nuclear-powered unmanned aircraft systems for military applications.',\n",
    "    'website': 'https://firestorm-defense.com',\n",
    "    'industry': 'Defense Manufacturing',\n",
    "    'size': '50-100 employees'\n",
    "}\n",
    "\n",
    "print(f\"ğŸ¢ Testing company: {test_company['name']}\")\n",
    "print(f\"ğŸ“ Description: {test_company['description']}\")\n",
    "\n",
    "try:\n",
    "    # Calculate score\n",
    "    scoring_result = scoring_engine.calculate_company_score(test_company)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š SCORING RESULTS:\")\n",
    "    print(f\"   ğŸ¯ Total Score: {scoring_result['total_score']:.1f}\")\n",
    "    print(f\"   ğŸ† Tier: {scoring_result['tier_classification']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Component Scores:\")\n",
    "    for component, score in scoring_result['component_scores'].items():\n",
    "        print(f\"   {component}: {score:.1f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¤ Keywords Found:\")\n",
    "    for category, keywords in scoring_result['keywords_found'].items():\n",
    "        if keywords:\n",
    "            print(f\"   {category}: {keywords}\")\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ Weighted Calculation:\")\n",
    "    for component, details in scoring_result['calculation_details'].items():\n",
    "        print(f\"   {component}: {details['raw_score']:.1f} Ã— {details['weight']} = {details['weighted_score']:.1f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Scoring Error: {str(e)}\")\n",
    "    print(f\"ğŸ” Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ End-to-End Workflow Testing\n",
    "\n",
    "Test the complete workflow with a single company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow test\n",
    "def test_single_company_workflow(company_name):\n",
    "    \"\"\"Test complete workflow for one company\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ COMPLETE WORKFLOW TEST: {company_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workflow_results = {\n",
    "        'company_name': company_name,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'steps': {}\n",
    "    }\n",
    "    \n",
    "    # Step 1: HigherGov Analysis\n",
    "    print(f\"\\nğŸ›¡ï¸ Step 1: Defense Contractor Analysis...\")\n",
    "    try:\n",
    "        defense_data = highergov_client.analyze_defense_contractor_status(company_name)\n",
    "        workflow_results['steps']['defense_analysis'] = {\n",
    "            'status': 'success',\n",
    "            'score': defense_data['defense_contractor_score'],\n",
    "            'contracts': defense_data['contract_analysis']['defense_contracts']\n",
    "        }\n",
    "        print(f\"   âœ… Defense Score: {defense_data['defense_contractor_score']}\")\n",
    "        print(f\"   ğŸ“‹ Contracts: {defense_data['contract_analysis']['defense_contracts']}\")\n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['defense_analysis'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   âŒ Error: {str(e)}\")\n",
    "    \n",
    "    # Step 2: OpenAI Research\n",
    "    print(f\"\\nğŸ¤– Step 2: AI Research...\")\n",
    "    try:\n",
    "        research_data = openai_client.conduct_research(\n",
    "            company_name=company_name,\n",
    "            research_type=\"basic_research\",\n",
    "            research_category=\"company_overview\"\n",
    "        )\n",
    "        workflow_results['steps']['ai_research'] = {\n",
    "            'status': 'success',\n",
    "            'tokens': research_data['metadata']['tokens_used'],\n",
    "            'content_length': len(research_data['content'])\n",
    "        }\n",
    "        print(f\"   âœ… Research completed: {research_data['metadata']['tokens_used']} tokens\")\n",
    "        print(f\"   ğŸ“„ Content preview: {research_data['content'][:150]}...\")\n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['ai_research'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   âŒ Error: {str(e)}\")\n",
    "    \n",
    "    # Step 3: Scoring\n",
    "    print(f\"\\nğŸ§® Step 3: Scoring Calculation...\")\n",
    "    try:\n",
    "        # Create company object for scoring\n",
    "        company_data = {\n",
    "            'name': company_name,\n",
    "            'description': research_data['content'] if 'research_data' in locals() else f\"Defense company: {company_name}\"\n",
    "        }\n",
    "        \n",
    "        scoring_result = scoring_engine.calculate_company_score(company_data)\n",
    "        workflow_results['steps']['scoring'] = {\n",
    "            'status': 'success',\n",
    "            'total_score': scoring_result['total_score'],\n",
    "            'tier': scoring_result['tier_classification']\n",
    "        }\n",
    "        print(f\"   âœ… Score: {scoring_result['total_score']:.1f}\")\n",
    "        print(f\"   ğŸ† Tier: {scoring_result['tier_classification']}\")\n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['scoring'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   âŒ Error: {str(e)}\")\n",
    "    \n",
    "    # Step 4: HubSpot Sync\n",
    "    print(f\"\\nğŸ“Š Step 4: HubSpot Sync...\")\n",
    "    try:\n",
    "        # Check if company exists\n",
    "        existing_companies = hubspot_client.search_companies({'name': company_name})\n",
    "        \n",
    "        hubspot_data = {\n",
    "            'name': company_name,\n",
    "            'atomus_score': scoring_result['total_score'] if 'scoring_result' in locals() else 0,\n",
    "            'tier_classification': scoring_result['tier_classification'] if 'scoring_result' in locals() else 'unknown',\n",
    "            'last_research_date': datetime.now().strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        if existing_companies:\n",
    "            # Update existing\n",
    "            company_id = existing_companies[0]['id']\n",
    "            hubspot_client.update_company(company_id, hubspot_data)\n",
    "            workflow_results['steps']['hubspot_sync'] = {\n",
    "                'status': 'updated',\n",
    "                'company_id': company_id\n",
    "            }\n",
    "            print(f\"   âœ… Updated existing company: {company_id}\")\n",
    "        else:\n",
    "            # Create new\n",
    "            new_company = hubspot_client.create_company(hubspot_data)\n",
    "            workflow_results['steps']['hubspot_sync'] = {\n",
    "                'status': 'created',\n",
    "                'company_id': new_company['id']\n",
    "            }\n",
    "            print(f\"   âœ… Created new company: {new_company['id']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['hubspot_sync'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   âŒ Error: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nğŸ“‹ WORKFLOW SUMMARY:\")\n",
    "    successful_steps = sum(1 for step in workflow_results['steps'].values() \n",
    "                          if step['status'] in ['success', 'created', 'updated'])\n",
    "    total_steps = len(workflow_results['steps'])\n",
    "    print(f\"   âœ… Successful steps: {successful_steps}/{total_steps}\")\n",
    "    \n",
    "    for step_name, step_data in workflow_results['steps'].items():\n",
    "        status_emoji = \"âœ…\" if step_data['status'] in ['success', 'created', 'updated'] else \"âŒ\"\n",
    "        print(f\"   {status_emoji} {step_name}: {step_data['status']}\")\n",
    "    \n",
    "    return workflow_results\n",
    "\n",
    "# Run the test\n",
    "test_result = test_single_company_workflow(\"Firestorm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Performance Monitoring\n",
    "\n",
    "Monitor API performance and usage statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Summary\n",
    "print(\"ğŸ“ˆ PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Show performance tracking results\n",
    "print(\"â±ï¸ Timing Results:\")\n",
    "timing_results = tracker.get_timing_summary()\n",
    "if timing_results:\n",
    "    for operation, times in timing_results.items():\n",
    "        avg_time = sum(times) / len(times) if times else 0\n",
    "        print(f\"   {operation}: {avg_time:.2f}s average ({len(times)} calls)\")\n",
    "else:\n",
    "    print(\"   No timing data recorded\")\n",
    "\n",
    "# API Usage Statistics\n",
    "print(\"\\nğŸ“Š API Usage Statistics:\")\n",
    "try:\n",
    "    print(\"\\nğŸ”µ HubSpot:\")\n",
    "    hubspot_client.log_stats_summary()\n",
    "    \n",
    "    print(\"\\nğŸ¤– OpenAI:\")\n",
    "    openai_client.log_stats_summary()\n",
    "    \n",
    "    print(\"\\nğŸ›¡ï¸ HigherGov:\")\n",
    "    highergov_client.log_stats_summary()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error getting API stats: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Data Exploration\n",
    "\n",
    "Explore the test dataset and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore test data\n",
    "print(\"ğŸ” TEST DATA EXPLORATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Load prospect database\n",
    "    test_companies = data_processor.load_prospect_database()\n",
    "    df = pd.DataFrame(test_companies)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset Overview:\")\n",
    "    print(f\"   Companies: {len(df)}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Show the data\n",
    "    print(f\"\\nğŸ“‹ Company List:\")\n",
    "    display(df)\n",
    "    \n",
    "    # Show data types and statistics\n",
    "    print(f\"\\nğŸ“ˆ Data Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration file exploration\n",
    "print(\"âš™ï¸ CONFIGURATION EXPLORATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Load scoring config\n",
    "    with open('../config/scoring_config.yaml', 'r') as f:\n",
    "        import yaml\n",
    "        scoring_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"ğŸ“ Scoring Configuration:\")\n",
    "    print(f\"   Weights: {scoring_config.get('weights', {})}\")\n",
    "    print(f\"   Tier Thresholds: {scoring_config.get('tier_thresholds', {})}\")\n",
    "    \n",
    "    # Load research prompts\n",
    "    with open('../config/research_prompts.yaml', 'r') as f:\n",
    "        research_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(f\"\\nğŸ¤– Research Prompts:\")\n",
    "    for category, prompts in research_config.items():\n",
    "        if isinstance(prompts, dict):\n",
    "            print(f\"   {category}: {len(prompts)} prompts\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading config: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Custom Testing Area\n",
    "\n",
    "Use this section for your own testing and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom testing area - modify as needed\n",
    "print(\"ğŸ› ï¸ CUSTOM TESTING AREA\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Example: Test specific company\n",
    "# company_to_test = \"Overland AI\"\n",
    "# Add your custom testing code here\n",
    "\n",
    "print(\"âœ… Ready for custom testing\")\n",
    "print(\"ğŸ’¡ Modify this cell to test specific functionality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}