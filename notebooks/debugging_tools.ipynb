{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atomus TAM Research - Debugging Tools\n",
    "\n",
    "This notebook provides interactive tools for testing and debugging the Atomus TAM Research system.\n",
    "\n",
    "## Features:\n",
    "- Individual API testing\n",
    "- Error diagnosis\n",
    "- Data exploration\n",
    "- Configuration validation\n",
    "- Performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our modules\n",
    "from api_integrations import (\n",
    "    create_hubspot_client,\n",
    "    create_openai_client, \n",
    "    create_highergov_client\n",
    ")\n",
    "from utils import (\n",
    "    get_logger,\n",
    "    get_performance_tracker,\n",
    "    log_system_info\n",
    ")\n",
    "from scoring_engine import AtomScoringEngine\n",
    "from data_processing import AtomDataProcessor\n",
    "\n",
    "print(\"✅ All imports successful\")\n",
    "print(f\"📁 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger and get system info\n",
    "logger = get_logger()\n",
    "tracker = get_performance_tracker()\n",
    "\n",
    "print(\"🖥️ SYSTEM INFORMATION:\")\n",
    "log_system_info()\n",
    "\n",
    "# Check environment variables\n",
    "print(\"\\n🔑 ENVIRONMENT VARIABLES:\")\n",
    "env_vars = {\n",
    "    'HUBSPOT_API_KEY': os.getenv('HUBSPOT_API_KEY', 'Not set'),\n",
    "    'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY', 'Not set'),\n",
    "    'HIGHERGOV_API_KEY': os.getenv('HIGHERGOV_API_KEY', 'Not set')\n",
    "}\n",
    "\n",
    "for key, value in env_vars.items():\n",
    "    if value == 'Not set':\n",
    "        print(f\"❌ {key}: {value}\")\n",
    "    else:\n",
    "        # Show only first 10 and last 4 characters for security\n",
    "        masked = f\"{value[:10]}...{value[-4:]}\" if len(value) > 14 else \"[MASKED]\"\n",
    "        print(f\"✅ {key}: {masked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Individual API Testing\n",
    "\n",
    "Test each API separately to identify any connection issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HubSpot API\n",
    "print(\"🔵 TESTING HUBSPOT API...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    hubspot_client = create_hubspot_client()\n",
    "    print(\"✅ HubSpot client created\")\n",
    "    \n",
    "    # Test connection\n",
    "    status = hubspot_client.test_connection()\n",
    "    print(f\"📊 Connection Status: {status}\")\n",
    "    \n",
    "    # Test searching for companies\n",
    "    print(\"\\n🔍 Testing company search...\")\n",
    "    search_results = hubspot_client.search_companies({'name': 'Test'})\n",
    "    print(f\"📋 Found {len(search_results)} companies matching 'Test'\")\n",
    "    \n",
    "    # Show current stats\n",
    "    print(\"\\n📈 HubSpot Stats:\")\n",
    "    hubspot_client.log_stats_summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ HubSpot API Error: {str(e)}\")\n",
    "    print(f\"🔍 Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI API\n",
    "print(\"🤖 TESTING OPENAI API...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    openai_client = create_openai_client()\n",
    "    print(\"✅ OpenAI client created\")\n",
    "    \n",
    "    # Test connection\n",
    "    status = openai_client.test_connection()\n",
    "    print(f\"📊 Connection Status: {status}\")\n",
    "    \n",
    "    # Test simple research\n",
    "    print(\"\\n🔍 Testing basic research...\")\n",
    "    test_research = openai_client.conduct_research(\n",
    "        company_name=\"Firestorm\",\n",
    "        research_type=\"basic_research\",\n",
    "        research_category=\"company_overview\"\n",
    "    )\n",
    "    \n",
    "    print(f\"📝 Research completed: {len(test_research['content'])} characters\")\n",
    "    print(f\"🎯 Tokens used: {test_research['metadata']['tokens_used']}\")\n",
    "    print(f\"📄 Preview: {test_research['content'][:200]}...\")\n",
    "    \n",
    "    # Show current stats\n",
    "    print(\"\\n📈 OpenAI Stats:\")\n",
    "    openai_client.log_stats_summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ OpenAI API Error: {str(e)}\")\n",
    "    print(f\"🔍 Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HigherGov API\n",
    "print(\"🛡️ TESTING HIGHERGOV API...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    highergov_client = create_highergov_client()\n",
    "    print(\"✅ HigherGov client created\")\n",
    "    \n",
    "    # Test connection\n",
    "    status = highergov_client.test_connection()\n",
    "    print(f\"📊 Connection Status: {status}\")\n",
    "    \n",
    "    # Test defense contractor analysis\n",
    "    print(\"\\n🔍 Testing defense contractor analysis...\")\n",
    "    defense_analysis = highergov_client.analyze_defense_contractor_status(\"Firestorm\")\n",
    "    \n",
    "    print(f\"📊 Defense Score: {defense_analysis['defense_contractor_score']}\")\n",
    "    print(f\"📋 Contract Count: {defense_analysis['contract_analysis']['defense_contracts']}\")\n",
    "    print(f\"🏢 Identifiers Found: {len(defense_analysis['identifiers'])}\")\n",
    "    \n",
    "    # Show current stats\n",
    "    print(\"\\n📈 HigherGov Stats:\")\n",
    "    highergov_client.log_stats_summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ HigherGov API Error: {str(e)}\")\n",
    "    print(f\"🔍 Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Configuration Testing\n",
    "\n",
    "Validate that all configuration files are properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Scoring Engine Configuration\n",
    "print(\"⚙️ TESTING SCORING ENGINE CONFIGURATION...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    scoring_engine = AtomScoringEngine()\n",
    "    print(\"✅ Scoring engine initialized\")\n",
    "    \n",
    "    # Show configuration summary\n",
    "    config = scoring_engine.get_config_summary()\n",
    "    print(f\"\\n📋 Configuration Summary:\")\n",
    "    print(f\"   📏 Scoring weights loaded: {len(config['weights'])} categories\")\n",
    "    print(f\"   🔤 Keyword categories: {len(config['keywords'])}\")\n",
    "    print(f\"   🎯 Tier thresholds: {config['tier_thresholds']}\")\n",
    "    \n",
    "    # Show keyword counts\n",
    "    print(f\"\\n🔤 Keyword Categories:\")\n",
    "    for category, keywords in config['keywords'].items():\n",
    "        print(f\"   {category}: {len(keywords)} keywords\")\n",
    "    \n",
    "    print(f\"\\n⚖️ Scoring Weights:\")\n",
    "    for category, weight in config['weights'].items():\n",
    "        print(f\"   {category}: {weight}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Scoring Engine Error: {str(e)}\")\n",
    "    print(f\"🔍 Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Processor\n",
    "print(\"📋 TESTING DATA PROCESSOR...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    data_processor = AtomDataProcessor()\n",
    "    print(\"✅ Data processor initialized\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = data_processor.load_prospect_database()\n",
    "    print(f\"📊 Loaded {len(test_data)} test companies\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if test_data:\n",
    "        print(f\"\\n📄 Sample company data:\")\n",
    "        sample = test_data[0]\n",
    "        for key, value in sample.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "            \n",
    "    # Test validation\n",
    "    validation_result = data_processor.validate_company_data(sample)\n",
    "    print(f\"\\n✅ Validation result: {validation_result['is_valid']}\")\n",
    "    if not validation_result['is_valid']:\n",
    "        print(f\"❌ Validation errors: {validation_result['errors']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Data Processor Error: {str(e)}\")\n",
    "    print(f\"🔍 Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧮 Interactive Scoring Testing\n",
    "\n",
    "Test the scoring engine with individual companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Company Scoring\n",
    "print(\"🧮 INTERACTIVE SCORING TESTING...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Define test company for scoring\n",
    "test_company = {\n",
    "    'name': 'Firestorm',\n",
    "    'description': 'Firestorm is a defense technology company specializing in hypersonic propulsion systems and nuclear-powered unmanned aircraft systems for military applications.',\n",
    "    'website': 'https://firestorm-defense.com',\n",
    "    'industry': 'Defense Manufacturing',\n",
    "    'size': '50-100 employees'\n",
    "}\n",
    "\n",
    "print(f\"🏢 Testing company: {test_company['name']}\")\n",
    "print(f\"📝 Description: {test_company['description']}\")\n",
    "\n",
    "try:\n",
    "    # Calculate score\n",
    "    scoring_result = scoring_engine.calculate_company_score(test_company)\n",
    "    \n",
    "    print(f\"\\n📊 SCORING RESULTS:\")\n",
    "    print(f\"   🎯 Total Score: {scoring_result['total_score']:.1f}\")\n",
    "    print(f\"   🏆 Tier: {scoring_result['tier_classification']}\")\n",
    "    \n",
    "    print(f\"\\n📈 Component Scores:\")\n",
    "    for component, score in scoring_result['component_scores'].items():\n",
    "        print(f\"   {component}: {score:.1f}\")\n",
    "    \n",
    "    print(f\"\\n🔤 Keywords Found:\")\n",
    "    for category, keywords in scoring_result['keywords_found'].items():\n",
    "        if keywords:\n",
    "            print(f\"   {category}: {keywords}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ Weighted Calculation:\")\n",
    "    for component, details in scoring_result['calculation_details'].items():\n",
    "        print(f\"   {component}: {details['raw_score']:.1f} × {details['weight']} = {details['weighted_score']:.1f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Scoring Error: {str(e)}\")\n",
    "    print(f\"🔍 Traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 End-to-End Workflow Testing\n",
    "\n",
    "Test the complete workflow with a single company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow test\n",
    "def test_single_company_workflow(company_name):\n",
    "    \"\"\"Test complete workflow for one company\"\"\"\n",
    "    \n",
    "    print(f\"🚀 COMPLETE WORKFLOW TEST: {company_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workflow_results = {\n",
    "        'company_name': company_name,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'steps': {}\n",
    "    }\n",
    "    \n",
    "    # Step 1: HigherGov Analysis\n",
    "    print(f\"\\n🛡️ Step 1: Defense Contractor Analysis...\")\n",
    "    try:\n",
    "        defense_data = highergov_client.analyze_defense_contractor_status(company_name)\n",
    "        workflow_results['steps']['defense_analysis'] = {\n",
    "            'status': 'success',\n",
    "            'score': defense_data['defense_contractor_score'],\n",
    "            'contracts': defense_data['contract_analysis']['defense_contracts']\n",
    "        }\n",
    "        print(f\"   ✅ Defense Score: {defense_data['defense_contractor_score']}\")\n",
    "        print(f\"   📋 Contracts: {defense_data['contract_analysis']['defense_contracts']}\")\n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['defense_analysis'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   ❌ Error: {str(e)}\")\n",
    "    \n",
    "    # Step 2: OpenAI Research\n",
    "    print(f\"\\n🤖 Step 2: AI Research...\")\n",
    "    try:\n",
    "        research_data = openai_client.conduct_research(\n",
    "            company_name=company_name,\n",
    "            research_type=\"basic_research\",\n",
    "            research_category=\"company_overview\"\n",
    "        )\n",
    "        workflow_results['steps']['ai_research'] = {\n",
    "            'status': 'success',\n",
    "            'tokens': research_data['metadata']['tokens_used'],\n",
    "            'content_length': len(research_data['content'])\n",
    "        }\n",
    "        print(f\"   ✅ Research completed: {research_data['metadata']['tokens_used']} tokens\")\n",
    "        print(f\"   📄 Content preview: {research_data['content'][:150]}...\")\n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['ai_research'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   ❌ Error: {str(e)}\")\n",
    "    \n",
    "    # Step 3: Scoring\n",
    "    print(f\"\\n🧮 Step 3: Scoring Calculation...\")\n",
    "    try:\n",
    "        # Create company object for scoring\n",
    "        company_data = {\n",
    "            'name': company_name,\n",
    "            'description': research_data['content'] if 'research_data' in locals() else f\"Defense company: {company_name}\"\n",
    "        }\n",
    "        \n",
    "        scoring_result = scoring_engine.calculate_company_score(company_data)\n",
    "        workflow_results['steps']['scoring'] = {\n",
    "            'status': 'success',\n",
    "            'total_score': scoring_result['total_score'],\n",
    "            'tier': scoring_result['tier_classification']\n",
    "        }\n",
    "        print(f\"   ✅ Score: {scoring_result['total_score']:.1f}\")\n",
    "        print(f\"   🏆 Tier: {scoring_result['tier_classification']}\")\n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['scoring'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   ❌ Error: {str(e)}\")\n",
    "    \n",
    "    # Step 4: HubSpot Sync\n",
    "    print(f\"\\n📊 Step 4: HubSpot Sync...\")\n",
    "    try:\n",
    "        # Check if company exists\n",
    "        existing_companies = hubspot_client.search_companies({'name': company_name})\n",
    "        \n",
    "        hubspot_data = {\n",
    "            'name': company_name,\n",
    "            'atomus_score': scoring_result['total_score'] if 'scoring_result' in locals() else 0,\n",
    "            'tier_classification': scoring_result['tier_classification'] if 'scoring_result' in locals() else 'unknown',\n",
    "            'last_research_date': datetime.now().strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        if existing_companies:\n",
    "            # Update existing\n",
    "            company_id = existing_companies[0]['id']\n",
    "            hubspot_client.update_company(company_id, hubspot_data)\n",
    "            workflow_results['steps']['hubspot_sync'] = {\n",
    "                'status': 'updated',\n",
    "                'company_id': company_id\n",
    "            }\n",
    "            print(f\"   ✅ Updated existing company: {company_id}\")\n",
    "        else:\n",
    "            # Create new\n",
    "            new_company = hubspot_client.create_company(hubspot_data)\n",
    "            workflow_results['steps']['hubspot_sync'] = {\n",
    "                'status': 'created',\n",
    "                'company_id': new_company['id']\n",
    "            }\n",
    "            print(f\"   ✅ Created new company: {new_company['id']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        workflow_results['steps']['hubspot_sync'] = {'status': 'failed', 'error': str(e)}\n",
    "        print(f\"   ❌ Error: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n📋 WORKFLOW SUMMARY:\")\n",
    "    successful_steps = sum(1 for step in workflow_results['steps'].values() \n",
    "                          if step['status'] in ['success', 'created', 'updated'])\n",
    "    total_steps = len(workflow_results['steps'])\n",
    "    print(f\"   ✅ Successful steps: {successful_steps}/{total_steps}\")\n",
    "    \n",
    "    for step_name, step_data in workflow_results['steps'].items():\n",
    "        status_emoji = \"✅\" if step_data['status'] in ['success', 'created', 'updated'] else \"❌\"\n",
    "        print(f\"   {status_emoji} {step_name}: {step_data['status']}\")\n",
    "    \n",
    "    return workflow_results\n",
    "\n",
    "# Run the test\n",
    "test_result = test_single_company_workflow(\"Firestorm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Performance Monitoring\n",
    "\n",
    "Monitor API performance and usage statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Summary\n",
    "print(\"📈 PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Show performance tracking results\n",
    "print(\"⏱️ Timing Results:\")\n",
    "timing_results = tracker.get_timing_summary()\n",
    "if timing_results:\n",
    "    for operation, times in timing_results.items():\n",
    "        avg_time = sum(times) / len(times) if times else 0\n",
    "        print(f\"   {operation}: {avg_time:.2f}s average ({len(times)} calls)\")\n",
    "else:\n",
    "    print(\"   No timing data recorded\")\n",
    "\n",
    "# API Usage Statistics\n",
    "print(\"\\n📊 API Usage Statistics:\")\n",
    "try:\n",
    "    print(\"\\n🔵 HubSpot:\")\n",
    "    hubspot_client.log_stats_summary()\n",
    "    \n",
    "    print(\"\\n🤖 OpenAI:\")\n",
    "    openai_client.log_stats_summary()\n",
    "    \n",
    "    print(\"\\n🛡️ HigherGov:\")\n",
    "    highergov_client.log_stats_summary()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error getting API stats: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Data Exploration\n",
    "\n",
    "Explore the test dataset and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore test data\n",
    "print(\"🔍 TEST DATA EXPLORATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Load prospect database\n",
    "    test_companies = data_processor.load_prospect_database()\n",
    "    df = pd.DataFrame(test_companies)\n",
    "    \n",
    "    print(f\"📊 Dataset Overview:\")\n",
    "    print(f\"   Companies: {len(df)}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Show the data\n",
    "    print(f\"\\n📋 Company List:\")\n",
    "    display(df)\n",
    "    \n",
    "    # Show data types and statistics\n",
    "    print(f\"\\n📈 Data Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration file exploration\n",
    "print(\"⚙️ CONFIGURATION EXPLORATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Load scoring config\n",
    "    with open('../config/scoring_config.yaml', 'r') as f:\n",
    "        import yaml\n",
    "        scoring_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"📏 Scoring Configuration:\")\n",
    "    print(f\"   Weights: {scoring_config.get('weights', {})}\")\n",
    "    print(f\"   Tier Thresholds: {scoring_config.get('tier_thresholds', {})}\")\n",
    "    \n",
    "    # Load research prompts\n",
    "    with open('../config/research_prompts.yaml', 'r') as f:\n",
    "        research_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(f\"\\n🤖 Research Prompts:\")\n",
    "    for category, prompts in research_config.items():\n",
    "        if isinstance(prompts, dict):\n",
    "            print(f\"   {category}: {len(prompts)} prompts\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading config: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Custom Testing Area\n",
    "\n",
    "Use this section for your own testing and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom testing area - modify as needed\n",
    "print(\"🛠️ CUSTOM TESTING AREA\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Example: Test specific company\n",
    "# company_to_test = \"Overland AI\"\n",
    "# Add your custom testing code here\n",
    "\n",
    "print(\"✅ Ready for custom testing\")\n",
    "print(\"💡 Modify this cell to test specific functionality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}